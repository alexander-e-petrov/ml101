{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ML101 Welcome to our training! In this training we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. About this training In this training you will be learning the basics of ML through IBM Cloud Pak for Data. Agenda Topic Description Type General Introduction ML101 Introduction Lecture Platform Overview Cloud Pak for Data overview Lecture AutoAI Environment provisioning and setup Hands-on lab Unstructured Data Analysis on R Studio RStudio overview Hands-on lab Custom Model Deployment Deploying a custom model using Jupyter notebooks Hands-on lab","title":"About the training"},{"location":"#welcome-to-ml101","text":"Welcome to our training! In this training we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications.","title":"Welcome to ML101"},{"location":"#about-this-training","text":"In this training you will be learning the basics of ML through IBM Cloud Pak for Data.","title":"About this training"},{"location":"#agenda","text":"Topic Description Type General Introduction ML101 Introduction Lecture Platform Overview Cloud Pak for Data overview Lecture AutoAI Environment provisioning and setup Hands-on lab Unstructured Data Analysis on R Studio RStudio overview Hands-on lab Custom Model Deployment Deploying a custom model using Jupyter notebooks Hands-on lab","title":"Agenda"},{"location":"resources/","text":"Documents and Resources by Section","title":"Resources"},{"location":"resources/#documents-and-resources-by-section","text":"","title":"Documents and Resources by Section"},{"location":"autoai/autoai/","text":"Automate model building with AutoAI In this module, we'll learn how to use AutoAI . The AutoAI tool is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. AutoAI automatically analyzes your data and generates candidate model pipelines customized for your predictive modeling problem. These model pipelines are created iteratively as AutoAI analyzes your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. This section is broken up into the following steps: Run AutoAI Experiment Save and Promote AutoAI Model Save AutoAI Notebooks Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. Create an AutoAI Experiment Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Give the AutoAI experiment any name you like. The associated Watson Machine Learning Service Instance should already be populated for you. If not, please select the one that you created in the setup section from the drop down. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. Click on the Select from project button to point to a dataset in your project. Now we can select the training CSV file. If you completed the previous Data Processing with Data Refinery lab module where you generated a single shaped CSV file from a refinery job, select that CSV file (the name will be whatever you selected in that module, for example: incident_shaped ). If you are presented with a prompt to create a time series forecast, click the No button. Now, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as inc_business_duration . AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. Click on the Data source tab and scroll down to the Select features to include section. Deselect the checkbox for the number column name. This will remove the incident number column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. ( Feel free to explore the other possible settings before clicking the save button ). To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes. Save and Promote AutoAI Model Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case, accuracy). Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 3 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name, add an optional description and tags, and click the Create button to save the model. You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space : Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification. Save AutoAI notebooks In addition to saving models, we now have the ability to save a Jupyter notebook with working code. This can be the code representation of the AutoAI experiment (with access to all of the generated pipelines), or it can be the implementation of a specific pipeline. Let's explore how we can save both the experiment and a pipeline as a notebook. To save the AutoAI experiment as a notebook, navigate back to the AutoAI experiment overview page by clicking on the AutoAI experiment name in your project assets page (under the AutoAI experiments section). Click on the Save code link in the 'Progress map' section of the UI. Leave the default name and click the Create button to save the experiment as a notebook. To save the AutoAI pipeline as a notebook, from the experiment overview page, scroll down to the pipeline leader board section. Then hover over the right side of one of the other pipelines and click the resulting Save as button. For sake of demonstration, we are going to save a pipeline for an algorithm that is different from the model we saved in the previous section (i.e in this case Pipeline 7). Choose the Notebook tile, accept the default name, add an optional description, and click the Create button. You will receive a notification to indicate that your notebook is saved to your project. Close the notification and then click on your project name in the project path at the top of the screen. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the instructions in the Modifying and running an AutoAI generated notebook lab for details. Conclusion Congratulation. We have now successfully created a highly optimized machine learning model using AutoAI and prepared it for deployment. In this section we covered one approach to building machine learning models on Cloud Pak for Data as a Service. We have seen how AutoAI helps to find an optimal model by automating tasks such as: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization","title":"Model building with AutoAI"},{"location":"autoai/autoai/#automate-model-building-with-autoai","text":"In this module, we'll learn how to use AutoAI . The AutoAI tool is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. AutoAI automatically analyzes your data and generates candidate model pipelines customized for your predictive modeling problem. These model pipelines are created iteratively as AutoAI analyzes your dataset and discovers data transformations, algorithms, and parameter settings that work best for your problem setting. This section is broken up into the following steps: Run AutoAI Experiment Save and Promote AutoAI Model Save AutoAI Notebooks Conclusion Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space.","title":"Automate model building with AutoAI"},{"location":"autoai/autoai/#create-an-autoai-experiment","text":"Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Give the AutoAI experiment any name you like. The associated Watson Machine Learning Service Instance should already be populated for you. If not, please select the one that you created in the setup section from the drop down. Then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. Click on the Select from project button to point to a dataset in your project. Now we can select the training CSV file. If you completed the previous Data Processing with Data Refinery lab module where you generated a single shaped CSV file from a refinery job, select that CSV file (the name will be whatever you selected in that module, for example: incident_shaped ). If you are presented with a prompt to create a time series forecast, click the No button. Now, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as inc_business_duration . AutoAI will set up defaults values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. To view/change these values, click the Experiment settings button. Click on the Data source tab and scroll down to the Select features to include section. Deselect the checkbox for the number column name. This will remove the incident number column from being used as a feature for the model. Although we could change other aspects of the experiment, we will accept the remaining default values and click the Save settings button. ( Feel free to explore the other possible settings before clicking the save button ). To start the experiment, click on the Run experiment button. The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes.","title":"Create an AutoAI Experiment"},{"location":"autoai/autoai/#save-and-promote-autoai-model","text":"Once the experiment completes, you can explore the various pipelines and options in the UI. Some of the options available are to see a comparison of the pipelines, to change the ranking based on a different performance metric, to see a log of the experiment, or to see the ranked listing of the pipelines (ranking based on the optimization metric in your experiment, in this case, accuracy). Scroll down to see the Pipeline leaderboard . The top performing pipeline is in the first rank. The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 3 gave the best result for our experiment. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard: The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. In order to deploy this model, click on the Save as button. On the next scren, select the Model option. Keep the default name, add an optional description and tags, and click the Create button to save the model. You will see a notification to indicate that your model is saved to your project. Click the View in project link in the notification to go to the saved model. ( Alternatively, if you navigate back to your project assets tab by closing the pipeline and AutoAI experiment, you will see the saved model in the Models section, which you can click on to explore ). To make the model available to be deployed, we need to make it available in the deployment space you created during the setup module. Click on the Promote to deployment space : Select the deployment space that was created as part of the setup module as the Target space and click Promote . You will be brought back to your project assets page and see a notification that the model was promoted to the deployment space succesfully. Feel free to close that notification.","title":"Save and Promote AutoAI Model"},{"location":"autoai/autoai/#save-autoai-notebooks","text":"In addition to saving models, we now have the ability to save a Jupyter notebook with working code. This can be the code representation of the AutoAI experiment (with access to all of the generated pipelines), or it can be the implementation of a specific pipeline. Let's explore how we can save both the experiment and a pipeline as a notebook. To save the AutoAI experiment as a notebook, navigate back to the AutoAI experiment overview page by clicking on the AutoAI experiment name in your project assets page (under the AutoAI experiments section). Click on the Save code link in the 'Progress map' section of the UI. Leave the default name and click the Create button to save the experiment as a notebook. To save the AutoAI pipeline as a notebook, from the experiment overview page, scroll down to the pipeline leader board section. Then hover over the right side of one of the other pipelines and click the resulting Save as button. For sake of demonstration, we are going to save a pipeline for an algorithm that is different from the model we saved in the previous section (i.e in this case Pipeline 7). Choose the Notebook tile, accept the default name, add an optional description, and click the Create button. You will receive a notification to indicate that your notebook is saved to your project. Close the notification and then click on your project name in the project path at the top of the screen. The notebook will be saved to your project, and can be examined in detail, changed and modified, and used to create a new model. See the instructions in the Modifying and running an AutoAI generated notebook lab for details.","title":"Save AutoAI notebooks"},{"location":"autoai/autoai/#conclusion","text":"Congratulation. We have now successfully created a highly optimized machine learning model using AutoAI and prepared it for deployment. In this section we covered one approach to building machine learning models on Cloud Pak for Data as a Service. We have seen how AutoAI helps to find an optimal model by automating tasks such as: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization","title":"Conclusion"},{"location":"autoai/data-refinery/","text":"Data Processing with Data Refinery In this module, we will prepare our data assets for analysis. We will use the Data Refinery graphical flow editor tool to create a set of ordered operations that will cleanse and shape our data. We will also explore the graphical interface to profile data and create visualizations to get a perspective and insights into the dataset. This section is broken up into the following steps: Merge and Cleanse Data Profile Data Visualize Data Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. Merge and Cleanse Data We will start by wrangling, shaping and refining our data. To do this, we will create a refinery flow to contain a series of data transformation steps. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To create a data refinery flow, click the Add to project button from the top of the page and click the Data Refinery flow option. Select Data assets on the left panel, then select the incident.csv data asset. Then click the Add button. The first thing we want to do is create a merged dataset. Start by joining the incident data with information about the incident SLA. Click the Operation button on the top left and then scroll down and select the Join operation. From the drop down list, select Inner join and then click the Add data set link to select the data asset you are going to join with. Select Data assets on the left panel and this time select the incident_sla.csv data asset. Then click the Apply button. Finish setting the following values and then click the Next button: Under the Source *Suffix option, enter _inc_ds . Under the Data set to join *Suffix option, enter _inc_sla_ds . Under the Join keys, click the input box and select number for incident.csv and inc_number for incident_sla.csv . Although we could modify what columns will be in the joined dataset, we will leave the default and include them all. Click the Apply button. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be useful features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the inc_calendar_duration column. For each column to be removed: Click the Operation + button, then select the Remove operation. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The column will be removed. Repeat the above two steps to remove the remaining six columns. Finally, we want to ensure there is no duplicates in our dataset. Click the Operation button once again and click the Remove duplicates operation. Select number as the column and click the Next button. Then click the Apply button in the subsequent panel. At this point, you have a data transformation flow with 11 steps. The flow keeps track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps and/or save for future use. Lets edit the flow name and output options. Click on the Information icon on the top right and then click the Edit button. Click the pencil icon next to Data Refinery Flow Name , set the name to incident_wrangling_cleaning_flow and click the Apply button. Then click the Edit Output pencil icon and set the name to incident_shaped.csv (leave the rest of the CSV output defaults) and click the 'Check mark icon'. Finally, click the Done button Click the Save icon to save the flow. Run Data Flow Job Data Refinery allows you to run these data flow jobs on demand or at scheduled times. In this way, you can regularly refine new data as it is updated. Click on the Jobs icon and then Save and create a job option from the menu. Give the job a name and optional description. Click the Next button. Click Next on the next two screens, leaving the default selections. You will reach the Review and create screen. Note the output name, which is incident_shaped . Click the Create and run button. When the job is successfully created, you will receive a notification. Click on the job details link in the notification panel to see the job status. The job will be listed with a status of Running and then the status will change to Completed . Once its completed, click the Edit configuration button. Click the pencil icon next to Schedule . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job. We will not run this as a job, so go ahead and click the Cancel link. Profile Data Go back to the project by clicking the name of the project in the breadcrumbs in the top left area of the browser. Click the Assets tab and then scroll down to the Data Refinery flows section and click on the incident_cleaning_flow flow. Wait for the flow operations to be applied and then click on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. Over three times more loan applicants have no checking than those with greater than 200 in checking. Visualize Data Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Add LoanAmount as the first column and click Add Column to add another column. Next add LoanDuration and click Visualize . The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left, click the Color Map section and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more risk (purple in this chart) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . Next on the left, select Risk in the Split By section, select the Stacked radio button, and uncheck the Show kde curve , as well as the Show distribution curve options. You should see a chart that looks like the following image (move the bin width down to 1 if necessary). It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the purple bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map (accept the warning if prompted). Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other . In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences ( Note: Hover over the icons to see the names ). Click on the gear icon in the Actions panel. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like. Finally, to save our plot as an image, click on the image icon on the top right, highlighted below, and then save the image. Conclusion We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Data processing with Data Refinery"},{"location":"autoai/data-refinery/#data-processing-with-data-refinery","text":"In this module, we will prepare our data assets for analysis. We will use the Data Refinery graphical flow editor tool to create a set of ordered operations that will cleanse and shape our data. We will also explore the graphical interface to profile data and create visualizations to get a perspective and insights into the dataset. This section is broken up into the following steps: Merge and Cleanse Data Profile Data Visualize Data Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space.","title":"Data Processing with Data Refinery"},{"location":"autoai/data-refinery/#merge-and-cleanse-data","text":"We will start by wrangling, shaping and refining our data. To do this, we will create a refinery flow to contain a series of data transformation steps. Go the (\u2630) navigation menu, expand Projects and click on the project you created during the setup section. To create a data refinery flow, click the Add to project button from the top of the page and click the Data Refinery flow option. Select Data assets on the left panel, then select the incident.csv data asset. Then click the Add button. The first thing we want to do is create a merged dataset. Start by joining the incident data with information about the incident SLA. Click the Operation button on the top left and then scroll down and select the Join operation. From the drop down list, select Inner join and then click the Add data set link to select the data asset you are going to join with. Select Data assets on the left panel and this time select the incident_sla.csv data asset. Then click the Apply button. Finish setting the following values and then click the Next button: Under the Source *Suffix option, enter _inc_ds . Under the Data set to join *Suffix option, enter _inc_sla_ds . Under the Join keys, click the input box and select number for incident.csv and inc_number for incident_sla.csv . Although we could modify what columns will be in the joined dataset, we will leave the default and include them all. Click the Apply button. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be useful features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the inc_calendar_duration column. For each column to be removed: Click the Operation + button, then select the Remove operation. In the Select column drop down, choose one of the columns to remove (i.e FirstName ). Click the Next button and then the Apply button. The column will be removed. Repeat the above two steps to remove the remaining six columns. Finally, we want to ensure there is no duplicates in our dataset. Click the Operation button once again and click the Remove duplicates operation. Select number as the column and click the Next button. Then click the Apply button in the subsequent panel. At this point, you have a data transformation flow with 11 steps. The flow keeps track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps and/or save for future use. Lets edit the flow name and output options. Click on the Information icon on the top right and then click the Edit button. Click the pencil icon next to Data Refinery Flow Name , set the name to incident_wrangling_cleaning_flow and click the Apply button. Then click the Edit Output pencil icon and set the name to incident_shaped.csv (leave the rest of the CSV output defaults) and click the 'Check mark icon'. Finally, click the Done button Click the Save icon to save the flow.","title":"Merge and Cleanse Data"},{"location":"autoai/data-refinery/#run-data-flow-job","text":"Data Refinery allows you to run these data flow jobs on demand or at scheduled times. In this way, you can regularly refine new data as it is updated. Click on the Jobs icon and then Save and create a job option from the menu. Give the job a name and optional description. Click the Next button. Click Next on the next two screens, leaving the default selections. You will reach the Review and create screen. Note the output name, which is incident_shaped . Click the Create and run button. When the job is successfully created, you will receive a notification. Click on the job details link in the notification panel to see the job status. The job will be listed with a status of Running and then the status will change to Completed . Once its completed, click the Edit configuration button. Click the pencil icon next to Schedule . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job. We will not run this as a job, so go ahead and click the Cancel link.","title":"Run Data Flow Job"},{"location":"autoai/data-refinery/#profile-data","text":"Go back to the project by clicking the name of the project in the breadcrumbs in the top left area of the browser. Click the Assets tab and then scroll down to the Data Refinery flows section and click on the incident_cleaning_flow flow. Wait for the flow operations to be applied and then click on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The median age of the applicants is 36, with the bulk under 49. About as many people had credits_paid_to_date as prior_payments_delayed. Few had no_credits. Over three times more loan applicants have no checking than those with greater than 200 in checking.","title":"Profile Data"},{"location":"autoai/data-refinery/#visualize-data","text":"Let's do some visual exploration of our data using charts and graphs. Note that this is an exploratory phase and we're looking for insights in out data. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up the page where you can select columns that you want to visualize. Add LoanAmount as the first column and click Add Column to add another column. Next add LoanDuration and click Visualize . The system will pick a suggested plot for you based on your data and show more suggested plot types at the top. Remember that we are most interested in knowing how these features impact a loan being at the risk. So, let's add the Risk as a color on top of our current scatter plot. That should help us visually see if there's something of interest here. From the left, click the Color Map section and select Risk . Also, to see the full data, drag the right side of the data selector at the bottom all the way to the right, in order to show all the data inside your plot. We notice that there are more risk (purple in this chart) on this plot towards the top right, than there is on the bottom left. This is a good start as it shows that there is probably a relationship between the riskiness of a loan and its duration and amount. It appears that the higher the amount and duration, the riskier the loan. Interesting, let's dig in further in how the loan duration could play into the riskiness of a loan. Let's plot a histogram of the LoanDuration to see if we can notice anything. First, select Histogram from the Chart Type . Next on the left, select Risk in the Split By section, select the Stacked radio button, and uncheck the Show kde curve , as well as the Show distribution curve options. You should see a chart that looks like the following image (move the bin width down to 1 if necessary). It looks like the longer the duration the larger the blue bar (risky loan count) become and the smaller the purple bars (non risky loan count) become. That indicate loans with longer duration are in general more likely to be risky. However, we need more information. We next explore if there is some insight in terms of the riskiness of a loan based on its duration when broken down by the loan purpose. To do so, let's create a Heat Map plot. At the top of the page, in the Chart Type section, open the arrows on the right, select Heat Map (accept the warning if prompted). Next, select Risk in the column section and LoanPurpose for the Row section. Additionally, to see the effects of the loan duration, select Mean in the summary section, and select LoanDuration in the Value section. You can now see that the least risky loans are those taken out for purchasing a new car and they are on average 10 years long. To the left of that cell we see that loans taken out for the same purpose that average around 15 years for term length seem to be more risky. So one could conclude the longer the loan term is, the more likely it will be risky. In contrast, we can see that both risky and non-risky loans for the other category seem to have the same average term length, so one could conclude that there's little, if any, relationship between loan length and its riskiness for the loans of type other . In general, for each row, the bigger the color difference between the right and left column, the more likely that loan duration plays a role for the riskiness of the loan category. Now let's look into customizing our plot. Under the Actions panel, notice that you can perform tasks such as Start over , Download chart details , Download chart image , or set Global visualization preferences ( Note: Hover over the icons to see the names ). Click on the gear icon in the Actions panel. We see that we can do things in the Global visualization preferences for Titles , Tools , Theme , and Notifications . Click on the Theme tab and update the color scheme to Dark . Then click the Apply button, now the colors for all of our charts will reflect this. Play around with various Themes and find one that you like. Finally, to save our plot as an image, click on the image icon on the top right, highlighted below, and then save the image.","title":"Visualize Data"},{"location":"autoai/data-refinery/#conclusion","text":"We've seen a some of the capabilities of the Data Refinery. We saw how we can transform data using R code, as well as using various operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profile the data, to see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot, histogram, and heatmap to explore the relationship between the riskiness of a loan and its duration, and purpose.","title":"Conclusion"},{"location":"autoai/servicenow-dataset/","text":"About the data set This data set contains incidents reported through ServiceNow since October 2020. incident.csv This CSV file contains incident dimension data. Field Description Type number Incident unique identifier String caller_id.employee_number Employee ID of the caller used to get the profile String u_on_behalf_of.employee_number Environment provisioning and setup String location Caller location address automatically added based on their profile. Can be modified based on the current location of the caller String u_floor Floor automatically added based on caller profile. Can be modified based on the current location of the caller String u_room Room automatically added based on caller profile. Can be modified based on the current location of the caller String category Used to categorize the incident. Examples: Hardware , Software , Inquiry/Help , etc. String subcategory The selected category define the options available for the subcategory . Examples: Clinical System , Access , etc. String business_service String cmdb_ci Impacted device String contact_type Defines the method the incident originated. Examples: Phone , Virtual Agent , etc. String parent String state Defines the lifecycle stage of the incident. Examples: In Progress , Resolved , Closed , etc. All incidents start at New String impact Defines the number of affected people or locations String urgency Defines how quickly the incident should be resolved String priority Provides guidance related to the sequence in which incidents need to be resolved and is automatically calculated based on impact and urgency selections String assignment_group Used to determine who will work on the incident String assigned_to When and individual is designated the assigned_to (not required to submit an incident), the incident moves to the state In Progress String short_description Used to describe the incident reported. May contain PHI String description Used to expand on the short_description . It should be used to add context to the issue reported. May contain PHI String close_code Resolution code. Example: Solved (Permanently) , Closed/Resolved by Caller String close_notes Resolution information. Defines what was done to resolve the incident String resolved_by.employee_number Employee ID that resolved the issue String resolved_at Time when the issues was marked as resolved Timestamp work_notes Updates to the form String incident_sla.csv Data extracted from the incident SLA fact table. Field Description Type inc_number Incident unique identifier. Can be used to join incident.number String taskslatable_sla Indicates response or resolution associated SLA String taskslatable_stage Define the lifecycle stage of the incident String taskslatable_has_breached Indicates if the response time has exceeded the SLA Boolean inc_business_duration Work time for the incident to be resolved in seconds Number inc_calendar_duration Calendar time for the incident to be resolved in seconds Number taskslatable_schedule Work schedule assigned to the incident String taskslatable_start_time Incident start time Timestamp taskslatable_end_time Incident end time Timestamp","title":"About the data set"},{"location":"autoai/servicenow-dataset/#about-the-data-set","text":"This data set contains incidents reported through ServiceNow since October 2020.","title":"About the data set"},{"location":"autoai/servicenow-dataset/#incidentcsv","text":"This CSV file contains incident dimension data. Field Description Type number Incident unique identifier String caller_id.employee_number Employee ID of the caller used to get the profile String u_on_behalf_of.employee_number Environment provisioning and setup String location Caller location address automatically added based on their profile. Can be modified based on the current location of the caller String u_floor Floor automatically added based on caller profile. Can be modified based on the current location of the caller String u_room Room automatically added based on caller profile. Can be modified based on the current location of the caller String category Used to categorize the incident. Examples: Hardware , Software , Inquiry/Help , etc. String subcategory The selected category define the options available for the subcategory . Examples: Clinical System , Access , etc. String business_service String cmdb_ci Impacted device String contact_type Defines the method the incident originated. Examples: Phone , Virtual Agent , etc. String parent String state Defines the lifecycle stage of the incident. Examples: In Progress , Resolved , Closed , etc. All incidents start at New String impact Defines the number of affected people or locations String urgency Defines how quickly the incident should be resolved String priority Provides guidance related to the sequence in which incidents need to be resolved and is automatically calculated based on impact and urgency selections String assignment_group Used to determine who will work on the incident String assigned_to When and individual is designated the assigned_to (not required to submit an incident), the incident moves to the state In Progress String short_description Used to describe the incident reported. May contain PHI String description Used to expand on the short_description . It should be used to add context to the issue reported. May contain PHI String close_code Resolution code. Example: Solved (Permanently) , Closed/Resolved by Caller String close_notes Resolution information. Defines what was done to resolve the incident String resolved_by.employee_number Employee ID that resolved the issue String resolved_at Time when the issues was marked as resolved Timestamp work_notes Updates to the form String","title":"incident.csv"},{"location":"autoai/servicenow-dataset/#incident_slacsv","text":"Data extracted from the incident SLA fact table. Field Description Type inc_number Incident unique identifier. Can be used to join incident.number String taskslatable_sla Indicates response or resolution associated SLA String taskslatable_stage Define the lifecycle stage of the incident String taskslatable_has_breached Indicates if the response time has exceeded the SLA Boolean inc_business_duration Work time for the incident to be resolved in seconds Number inc_calendar_duration Calendar time for the incident to be resolved in seconds Number taskslatable_schedule Work schedule assigned to the incident String taskslatable_start_time Incident start time Timestamp taskslatable_end_time Incident end time Timestamp","title":"incident_sla.csv"},{"location":"autoai/wml/","text":"Machine Learning Model Online Deployment and Scoring In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Online Deployments - Allows you to run the model on data in real-time, as data is received by a web service. This lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section . Create Online Model Deployment After a model has been created, saved and promoted to our deployment space, we can proceed to deploying the model. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Models' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the Create a deployment screen, choose Online for the Deployment Type , give the Deployment a name and optionally a description and click the Create button. Click on the Deployments tab. The new deployment status will show as In progress and then switch to Deployed when it is complete. Test Online Model Deployment Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or the SparkML one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [.......] } that may already be in the test input panel. === \"AutoAI Model\" ```json { \"input_data\": [ { \"fields\": [ \"CustomerID\", \"LoanDuration\", \"LoanPurpose\", \"LoanAmount\", \"InstallmentPercent\", \"OthersOnLoan\", \"EmploymentDuration\", \"Sex\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"Housing\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\", \"CheckingStatus\", \"CreditHistory\", \"ExistingSavings\", \"InstallmentPlans\", \"ExistingCreditsCount\" ], \"values\": [ [ \"1\", 13, \"car_new\", 1343, 2, \"none\", \"1_to_4\", \"female\", 3, \"savings_insurance\", 46, \"own\", \"skilled\", 1, \"none\", \"yes\", \"no_checking\", \"credits_paid_to_date\", \"100_to_500\", \"none\", 2 ] ] } ] } ``` === \"Jupyter Spark Model\" ```json { \"input_data\": [{ \"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"], \"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]] }]} ``` Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"). Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be present or will remain grayed out. Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below. (Optional) Test Online Model Deployment using cURL Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. For simplicity, we will be using the IBM Cloud shell to run these cURL commands. If you have the IBM Cloud CLI and associated command line utilities installed locally, you are free to attempt these steps on your own machine. In a new browser window/tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar. The cloud shell will launch a new browser tab with a web terminal. In order to invoke the model endpoints, we need to authorize using an access token. To get an access token you will use the IBM Cloud API Key , which you created during the setup section. In the cloud shell window, run the following command to get a token to access the API. Replace <API Key> with the api key that you got from running above command. bash curl -X POST 'https://iam.cloud.ibm.com/identity/token' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: application/json' --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' --data-urlencode 'apikey=<API Key>' A json string will be returned with a value for accessToken that will look similar to this: json { \"access_token\":\"AAAAAAAfakeACCESSTOKENNNNNNN\", \"refresh_token\":\"BBBBBBBBBBBFAKEREFRESHTOKENNNNNNNNNNNNN\", \"token_type\":\"Bearer\", \"expires_in\":3600, \"expiration\":1601317201, \"scope\":\"ibm openid\" } You will save the access token value shown after the access_token key in a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . bash export WML_AUTH_TOKEN=<value-of-access-token> Back on the Cloud Pak for Data model deployment page, go to the API reference tab and copy the Endpoint value (an example endpoint would be: \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments/<DEPLOYMENT_ID>/predictions?version=2020-09-01\" ). In your cloud shell terminal, save that endpoint to a variable named URL by exporting the value. ( Note: the URL should end with a version query parameter ). bash export WML_URL=<value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: bash curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN\" -d '{\"input_data\": [{\"fields\": [\"CustomerID\",\"LoanDuration\",\"LoanPurpose\",\"LoanAmount\",\"InstallmentPercent\",\"OthersOnLoan\",\"EmploymentDuration\",\"Sex\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"Housing\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\",\"CheckingStatus\",\"CreditHistory\",\"ExistingSavings\",\"InstallmentPlans\",\"ExistingCreditsCount\"],\"values\": [[\"1\",13,\"car_new\",1343,2,\"none\",\"1_to_4\",\"female\",3,\"savings_insurance\",46,\"own\",\"skilled\",1,\"none\",\"yes\",\"no_checking\",\"credits_paid_to_date\",\"100_to_500\",\"none\",2]]}]}' $WML_URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk). Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below. Cleanup Deployments You can clean up the deployments created for your models. To remove the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Model' table, click on the model name that you previousely promoted and created deployments against. Under 'Deployment Types', click on Online to view the online deployments you have created for this model. In the table on the main panel, click on the three vertical dots at the right of the row for the online deployment you created. Select the Delete option from the menu. Note: The vertical dots are hidden until you hover over them with your mouse In the subsequent pop up window, click on the Delete button to confirm you want to delete this deployment. You can follow the same process to delete other deployments as needed. Conclusion Congratulations. You've completed this lab and seen how to create and test online deployments for your machine learning models. Feel free to explore the additional (optional) deployment labs, which show you how to: Create and test a batch deployments. Integrate the online deployment to an application.","title":"Deploying and consuming models"},{"location":"autoai/wml/#machine-learning-model-online-deployment-and-scoring","text":"In this module, we will learn how to deploy our Machine Learning models. By doing so, we make them available for use in production such that applications and business processes can derive insights from them. There are several types of deployments available ( depending on the model framework used ). In this lab, we will explore: Online Deployments - Allows you to run the model on data in real-time, as data is received by a web service. This lab will build an online deployment and test the model endpoint using both the built in testing tool as well as external testing tools. Note: You can click on any image in the instructions below to zoom in and see more details. When you do that just click on your browser's back button to return to the previous page. Note: The lab instructions below assume you have completed the setup section already, if not, be sure to complete the setup first to create a project and a deployment space. If you would have any issues, check the FAQ section .","title":"Machine Learning Model Online Deployment and Scoring"},{"location":"autoai/wml/#create-online-model-deployment","text":"After a model has been created, saved and promoted to our deployment space, we can proceed to deploying the model. For this section, we will be creating an online deployment. This type of deployment will make an instance of the model available to make predictions in real time via an API. Although we will use the Cloud Pak for Data UI to deploy the model, the same can be done programmatically. Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Click on the Spaces tab and then choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Models' table, find the model name for the model you previously built and now want to create a deployment against. Use your mouse to hover over the right side of that table row and click the Deploy rocket icon (the icons are not visible by default until you hover over them). Note: There may be more than one model listed in them 'Models' section. This can happen if you have run the Jupyter notebook that creates SparkML models more than once or if you have run through both the Jupyter notebook and AutoAI modules to create models. On the Create a deployment screen, choose Online for the Deployment Type , give the Deployment a name and optionally a description and click the Create button. Click on the Deployments tab. The new deployment status will show as In progress and then switch to Deployed when it is complete.","title":"Create Online Model Deployment"},{"location":"autoai/wml/#test-online-model-deployment","text":"Cloud Pak for Data offers tools to quickly test out Watson Machine Learning models. We begin with the built-in tooling. From the Model deployment page, once the deployment status shows as Deployed , click on the name of your deployment. The deployment API reference tab shows how to use the model using cURL , Java , Javascript , Python , and Scala . To get to the built-in test tool, click on the Test tab and then click on the Provide input data as JSON icon. Copy and paste the following data objects into the Body panel (replace the text that was in the input panel). Note: Click the tab appropriate for the model you are testing (either an AutoAI model or the SparkML one built using the Jupyter notebook). Also make sure the input below is the only content in the field. Do not append it to the default content { \"input_data\": [.......] } that may already be in the test input panel. === \"AutoAI Model\" ```json { \"input_data\": [ { \"fields\": [ \"CustomerID\", \"LoanDuration\", \"LoanPurpose\", \"LoanAmount\", \"InstallmentPercent\", \"OthersOnLoan\", \"EmploymentDuration\", \"Sex\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"Housing\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\", \"CheckingStatus\", \"CreditHistory\", \"ExistingSavings\", \"InstallmentPlans\", \"ExistingCreditsCount\" ], \"values\": [ [ \"1\", 13, \"car_new\", 1343, 2, \"none\", \"1_to_4\", \"female\", 3, \"savings_insurance\", 46, \"own\", \"skilled\", 1, \"none\", \"yes\", \"no_checking\", \"credits_paid_to_date\", \"100_to_500\", \"none\", 2 ] ] } ] } ``` === \"Jupyter Spark Model\" ```json { \"input_data\": [{ \"fields\": [ \"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\", \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\", \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\", \"Telephone\", \"ForeignWorker\"], \"values\": [[ \"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3, \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"]] }]} ``` Click the Predict button. The model will be called with the input data and the results will display in the Result window. Scroll down to the bottom of the result to see the prediction (i.e \"Risk\" or \"No Risk\"). Note: For some deployed models (for example AutoAI based models), you can provide the request payload using a generated form by clicking on the Provide input using form icon and providing values for the input fields of the form. If the form is not available for the model you deployed, the icon will not be present or will remain grayed out. Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below.","title":"Test Online Model Deployment"},{"location":"autoai/wml/#optional-test-online-model-deployment-using-curl","text":"Now that the model is deployed, we can also test it from external applications. One way to invoke the model API is using the cURL command. For simplicity, we will be using the IBM Cloud shell to run these cURL commands. If you have the IBM Cloud CLI and associated command line utilities installed locally, you are free to attempt these steps on your own machine. In a new browser window/tab, go to the IBM Cloud Home Page and click the terminal icon in the upper right-hand bar. The cloud shell will launch a new browser tab with a web terminal. In order to invoke the model endpoints, we need to authorize using an access token. To get an access token you will use the IBM Cloud API Key , which you created during the setup section. In the cloud shell window, run the following command to get a token to access the API. Replace <API Key> with the api key that you got from running above command. bash curl -X POST 'https://iam.cloud.ibm.com/identity/token' -H 'Content-Type: application/x-www-form-urlencoded' -H 'Accept: application/json' --data-urlencode 'grant_type=urn:ibm:params:oauth:grant-type:apikey' --data-urlencode 'apikey=<API Key>' A json string will be returned with a value for accessToken that will look similar to this: json { \"access_token\":\"AAAAAAAfakeACCESSTOKENNNNNNN\", \"refresh_token\":\"BBBBBBBBBBBFAKEREFRESHTOKENNNNNNNNNNNNN\", \"token_type\":\"Bearer\", \"expires_in\":3600, \"expiration\":1601317201, \"scope\":\"ibm openid\" } You will save the access token value shown after the access_token key in a temporary environment variable in your terminal. Copy the access token value (without the quotes) in the terminal and then use the following export command to save the \"accessToken\" to a variable called WML_AUTH_TOKEN . bash export WML_AUTH_TOKEN=<value-of-access-token> Back on the Cloud Pak for Data model deployment page, go to the API reference tab and copy the Endpoint value (an example endpoint would be: \"https://us-south.ml.cloud.ibm.com/ml/v4/deployments/<DEPLOYMENT_ID>/predictions?version=2020-09-01\" ). In your cloud shell terminal, save that endpoint to a variable named URL by exporting the value. ( Note: the URL should end with a version query parameter ). bash export WML_URL=<value-of-endpoint> Now run this curl command from the terminal to invoke the model with the same payload we used previousy: bash curl -k -X POST --header 'Content-Type: application/json' --header 'Accept: application/json' --header \"Authorization: Bearer $WML_AUTH_TOKEN\" -d '{\"input_data\": [{\"fields\": [\"CustomerID\",\"LoanDuration\",\"LoanPurpose\",\"LoanAmount\",\"InstallmentPercent\",\"OthersOnLoan\",\"EmploymentDuration\",\"Sex\",\"CurrentResidenceDuration\",\"OwnsProperty\",\"Age\",\"Housing\",\"Job\",\"Dependents\",\"Telephone\",\"ForeignWorker\",\"CheckingStatus\",\"CreditHistory\",\"ExistingSavings\",\"InstallmentPlans\",\"ExistingCreditsCount\"],\"values\": [[\"1\",13,\"car_new\",1343,2,\"none\",\"1_to_4\",\"female\",3,\"savings_insurance\",46,\"own\",\"skilled\",1,\"none\",\"yes\",\"no_checking\",\"credits_paid_to_date\",\"100_to_500\",\"none\",2]]}]}' $WML_URL A json string will be returned with the response, including a prediction from the model (i.e a \"Risk\" or \"No Risk\" at the end indicating the prediction of this loan representing risk). Important: If you have completed this section and do not plan on completing the other optional deployment approaches below, please go ahead and cleanup your deployment. Follow the Cleanup Deployment instructions below.","title":"(Optional) Test Online Model Deployment using cURL"},{"location":"autoai/wml/#cleanup-deployments","text":"You can clean up the deployments created for your models. To remove the deployment: Navigate to the left-hand (\u2630) hamburger menu, expand the Deployments section and click on View all spaces . Choose the deployment space you setup previously by clicking on the name of your space. From your deployment space overview, click the Assets tab and in the 'Model' table, click on the model name that you previousely promoted and created deployments against. Under 'Deployment Types', click on Online to view the online deployments you have created for this model. In the table on the main panel, click on the three vertical dots at the right of the row for the online deployment you created. Select the Delete option from the menu. Note: The vertical dots are hidden until you hover over them with your mouse In the subsequent pop up window, click on the Delete button to confirm you want to delete this deployment. You can follow the same process to delete other deployments as needed.","title":"Cleanup Deployments"},{"location":"autoai/wml/#conclusion","text":"Congratulations. You've completed this lab and seen how to create and test online deployments for your machine learning models. Feel free to explore the additional (optional) deployment labs, which show you how to: Create and test a batch deployments. Integrate the online deployment to an application.","title":"Conclusion"},{"location":"jupyter/custom-model-deployment/","text":"","title":"Predict incident resolution time with Watson Studio Jupyter notebooks"},{"location":"rstudio/unstructured-data-analysis-on-rstudio/","text":"RStudio for unstructured data and Introductory NLP with Cloud Pak for Data Working with R in RStudio within CPD and using examples from Pubmed, an analysis of text with the LDA (Local Dirichlet Allocation) topic modeling algorithm will be presented by John Cadley and Luke Czapla. This session is accessible to both beginners and experts with the R programming language, through providing code and modifying key terms for the search queries. The visualization of resulting clusters, their importance, and intersection of cluster words within other clusters will then be done in RStudio as well, with a browseable visualization tool. Subtopics and Agenda: Initialization of an RStudio environment with small resource allocation. Syntax for Pubmed queries Processes for returning results using Entrez Direct Running demonstration LDA topic modeling over Pubmed abstracts Additional topics Additional Topics (as time allows) Document-level clustering with Carrot2, a web interface for clustering any set of documents with cluster topics titles based on any desired key fields. Discussion of STC, Lingo, and Bisecting K-means algorithms.","title":"Unstructured Data Analysis on RStudio"},{"location":"rstudio/unstructured-data-analysis-on-rstudio/#rstudio-for-unstructured-data-and-introductory-nlp-with-cloud-pak-for-data","text":"Working with R in RStudio within CPD and using examples from Pubmed, an analysis of text with the LDA (Local Dirichlet Allocation) topic modeling algorithm will be presented by John Cadley and Luke Czapla. This session is accessible to both beginners and experts with the R programming language, through providing code and modifying key terms for the search queries. The visualization of resulting clusters, their importance, and intersection of cluster words within other clusters will then be done in RStudio as well, with a browseable visualization tool.","title":"RStudio for unstructured data and Introductory NLP with Cloud Pak for Data"},{"location":"rstudio/unstructured-data-analysis-on-rstudio/#subtopics-and-agenda","text":"Initialization of an RStudio environment with small resource allocation. Syntax for Pubmed queries Processes for returning results using Entrez Direct Running demonstration LDA topic modeling over Pubmed abstracts Additional topics","title":"Subtopics and Agenda:"},{"location":"rstudio/unstructured-data-analysis-on-rstudio/#additional-topics-as-time-allows","text":"Document-level clustering with Carrot2, a web interface for clustering any set of documents with cluster topics titles based on any desired key fields. Discussion of STC, Lingo, and Bisecting K-means algorithms.","title":"Additional Topics (as time allows)"}]}